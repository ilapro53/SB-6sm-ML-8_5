{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/o/study/ml_6/8.5/custom_server\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive\t\t      requirements.txt\n",
      "build_client.ipynb    server.log\n",
      "install_server.sh     server_config.json\n",
      "my_model.keras\t      start_server.sh\n",
      "prepare_env.sh\t      tensorflow-model-server-universal_2.8.0_all.deb\n",
      "prepare_model.ipynb   test.png\n",
      "prepare_server.ipynb  test_images\n"
     ]
    }
   ],
   "source": [
    "!ls ../local/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./server_models/my_model/1/\n",
    "!cp ../local/my_model.keras ./server_models/my_model/1/my_model.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:25:19.596397: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 13:25:19.600358: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 13:25:20.783216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "from markupsafe import escape\n",
    "from gevent.pywsgi import WSGIServer\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server.py\n",
    "from flask import Flask, request\n",
    "from markupsafe import escape\n",
    "from gevent.pywsgi import WSGIServer\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print('Current server path:', Path().resolve())\n",
    "models_folder = Path(\"./server_models\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def load_model(name, version):\n",
    "    path = models_folder / name / str(version)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        f = [p for p in os.listdir(path) if p.endswith(\".keras\")]\n",
    "        if len(f) == 1:\n",
    "            return tf.keras.models.load_model(path / f[0])\n",
    "\n",
    "\n",
    "def load_best_model(model_name):\n",
    "    versions = os.listdir(models_folder / model_name)\n",
    "\n",
    "    rg = re.compile(r\"\\d+\")\n",
    "    versions = set([int(s) for s in versions if rg.match(s)])\n",
    "    versions = list([s for s in versions if str(s) == str(int(s))])\n",
    "    versions.sort(reverse=True)\n",
    "\n",
    "    for ver in versions:\n",
    "        try:\n",
    "            model = load_model(model_name, ver)\n",
    "            if not (model is None):\n",
    "                return model\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    raise Exception(\"Model not found or can't load any version of model\")\n",
    "\n",
    "\n",
    "def best_model_predict(model_name, input_data):\n",
    "    model = load_best_model(model_name)\n",
    "\n",
    "    pred_data = model.predict(input_data)\n",
    "    return pred_data\n",
    "\n",
    "\n",
    "@app.route(\"/models/<model>/:predict\", methods=[\"POST\"])\n",
    "def main(model):\n",
    "\n",
    "    try:\n",
    "        ## model ##\n",
    "\n",
    "        data = json.loads(request.data)\n",
    "\n",
    "        if not (model in os.listdir(models_folder)):\n",
    "            return {\"error\": f\"not found model: {model}\"}\n",
    "\n",
    "        ## version ##\n",
    "\n",
    "        versions = os.listdir(models_folder / model)\n",
    "\n",
    "        rg = re.compile(r\"\\d+\")\n",
    "        versions = set([int(s) for s in versions if rg.match(s)])\n",
    "        versions = set([s for s in versions if str(s) == str(int(s))])\n",
    "\n",
    "        if len(versions) != 0:\n",
    "            version = str(max(versions))\n",
    "        else:\n",
    "            return {\"error\": f\"no versions found for model \\\"{model}\\\"\"}\n",
    "\n",
    "        # model = load_best_model(model)\n",
    "\n",
    "        if not ('instances' in data):\n",
    "            raise Exception('No field \"instances\" found in your data')\n",
    "        \n",
    "        data = np.array(data['instances'])\n",
    "\n",
    "        pred = best_model_predict(\n",
    "            model_name=model, \n",
    "            input_data=data\n",
    "        )\n",
    "        \n",
    "        return {'prdictions': pred.tolist()}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # app.run(port=8000)\n",
    "    http_server = WSGIServer((\"\", 8502), app)\n",
    "    print(\"Server is running. Ctrl+C to interrupt.\")\n",
    "    http_server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:35:37.548941: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 13:35:37.552482: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 13:35:38.911915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Current server path: /mnt/o/study/ml_6/8.5/custom_server\n",
      "Server is running. Ctrl+C to interrupt.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:06] \"POST /models/my_model/:predict HTTP/1.1\" 200 323 0.328483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:07] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.232070\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:07] \"POST /models/my_model/:predict HTTP/1.1\" 200 318 0.228499\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:08] \"POST /models/my_model/:predict HTTP/1.1\" 200 317 0.207093\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1e1c30fba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:08] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.235869\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1e1c2d32e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:09] \"POST /models/my_model/:predict HTTP/1.1\" 200 316 0.215599\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:10] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.244885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:10] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.206188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:11] \"POST /models/my_model/:predict HTTP/1.1\" 200 321 0.224307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:36:12] \"POST /models/my_model/:predict HTTP/1.1\" 200 321 0.208322\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:48] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001116\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:48] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001188\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:49] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001019\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:49] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001003\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:49] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001256\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:50] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.000949\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:50] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.000948\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:51] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.001551\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:51] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.000908\n",
      "[Errno 2] No such file or directory: 'server_models'\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:55:51] \"POST /models/my_model/:predict HTTP/1.1\" 200 173 0.000981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:08] \"POST /models/my_model/:predict HTTP/1.1\" 200 323 0.612889\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:13] \"POST /models/my_model/:predict HTTP/1.1\" 200 323 0.229055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:14] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.193869\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:15] \"POST /models/my_model/:predict HTTP/1.1\" 200 318 0.178953\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:15] \"POST /models/my_model/:predict HTTP/1.1\" 200 317 0.189596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:16] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.213459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:16] \"POST /models/my_model/:predict HTTP/1.1\" 200 316 0.216980\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:17] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.231176\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:18] \"POST /models/my_model/:predict HTTP/1.1\" 200 319 0.229573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:18] \"POST /models/my_model/:predict HTTP/1.1\" 200 321 0.211300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "::ffff:127.0.0.1 - - [2024-05-27 13:58:19] \"POST /models/my_model/:predict HTTP/1.1\" 200 321 0.196725\n",
      "^C\n",
      "KeyboardInterrupt\n",
      "2024-05-27T10:58:44Z\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/o/study/ml_6/8.5/custom_server/server.py\", line 98, in <module>\n",
      "    http_server.serve_forever()\n",
      "  File \"/home/ilapr/anaconda3/lib/python3.11/site-packages/gevent/baseserver.py\", line 400, in serve_forever\n",
      "    self._stop_event.wait()\n",
      "  File \"src/gevent/event.py\", line 163, in gevent._gevent_cevent.Event.wait\n",
      "  File \"src/gevent/_abstract_linkable.py\", line 521, in gevent._gevent_c_abstract_linkable.AbstractLinkable._wait\n",
      "  File \"src/gevent/_abstract_linkable.py\", line 487, in gevent._gevent_c_abstract_linkable.AbstractLinkable._wait_core\n",
      "  File \"src/gevent/_abstract_linkable.py\", line 490, in gevent._gevent_c_abstract_linkable.AbstractLinkable._wait_core\n",
      "  File \"src/gevent/_abstract_linkable.py\", line 442, in gevent._gevent_c_abstract_linkable.AbstractLinkable._AbstractLinkable__wait_to_be_notified\n",
      "  File \"src/gevent/_abstract_linkable.py\", line 451, in gevent._gevent_c_abstract_linkable.AbstractLinkable._switch_to_hub\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 65, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_gevent_c_greenlet_primitives.pxd\", line 35, in gevent._gevent_c_greenlet_primitives._greenlet_switch\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
